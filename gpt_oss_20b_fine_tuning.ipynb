{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithinp1/Mindfull-GPT/blob/main/gpt_oss_20b_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_section"
      },
      "source": [
        "# Fine-tuning GPT-OSS (20B) as a MindGPT ‚Äî A Mental Well-Being Assistant Model with Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "why_section"
      },
      "source": [
        "In this notebook, we'll walk through how a powerful open-source model like `Llama-3-8B` can be fine-tuned to act as **an empathetic mental well-being assistant**. We'll do this by:\n",
        "\n",
        "1.  Fine-tuning with [Unsloth](https://github.com/unslothai/unsloth) for 2x faster training and reduced memory usage.\n",
        "2.  Using a curated dataset of empathetic conversations, anonymized therapy dialogues, and mindfulness exercises.\n",
        "\n",
        "The end result? A model that can offer a supportive ear, guide users through simple grounding techniques, and provide helpful reframing exercises based on Cognitive Behavioral Therapy (CBT) principles.\n",
        "\n",
        "Here's an example of how **MindGPT** might interact with a user:\n",
        "\n",
        "```\n",
        "User: I'm feeling really overwhelmed with my deadlines at work. I don't know where to even start.\n",
        "\n",
        "Assistant's Internal Strategy:\n",
        "1.  Acknowledge and validate the user's feelings of being overwhelmed.\n",
        "2.  Express empathy for their situation.\n",
        "3.  Suggest a simple, actionable technique to reduce immediate stress (breathing exercise).\n",
        "4.  Gently guide the user through the steps.\n",
        "\n",
        "Assistant's Response:\n",
        "It sounds like you're under a lot of pressure right now, and feeling overwhelmed when facing tight deadlines is completely understandable. It can be hard to think clearly when that stress builds up.\n",
        "\n",
        "Before diving into the tasks, would you be open to trying a quick 30-second breathing exercise to help calm your nervous system? It might make it easier to figure out a starting point.\n",
        "```\n",
        "\n",
        "This project aims to leverage AI to make initial mental well-being support more accessible and to provide users with practical tools for managing daily stress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_section"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!\n",
        "!pip install --upgrade -qqq uv\n",
        "try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "except: get_numpy = \"numpy\"\n",
        "!uv pip install -qqq \\\n",
        "    \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \\\n",
        "    \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "    \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "    git+https://github.com/huggingface/transformers \\\n",
        "    git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loading_section"
      },
      "source": [
        "### Loading the Model with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "bf3bdbf2-fbd3-4b62-c2a1-eeed0e36d7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Direct module loading failed for unsloth_compiled_module_gpt_oss: name 'KWARGS_TYPE' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mcreate_new_function\u001b[0;34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mnew_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(compile_folder, name)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Try standard import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mnew_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/unsloth_compiled_module_gpt_oss.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKWARGS_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m ) -> tuple[torch.Tensor, torch.Tensor]:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KWARGS_TYPE' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mcreate_new_function\u001b[0;34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/tmp/unsloth_compiled_cache/unsloth_compiled_module_gpt_oss.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKWARGS_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m ) -> tuple[torch.Tensor, torch.Tensor]:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KWARGS_TYPE' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36munsloth_compile_transformers\u001b[0;34m(model_type, sdpa_dynamic_mask, sdpa_bool_masks, sdpa_gqa_replace, sdpa_dynamic_compile, compile_attention, disable_causal_masks, compile_torch_modules, compile_custom_modules, compile_function_calls, fuse_lm_head, gradient_checkpointing, manual_replacements, fast_lora_forwards, fast_residual_stream, accurate_accumulation, epilogue_fusion, max_autotune, shape_padding, cudagraphs, debug, fullgraph, import_from_cache, disable, return_logits, supports_sdpa)\u001b[0m\n\u001b[1;32m   2617\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m         combined_module = create_new_function(\n\u001b[0m\u001b[1;32m   2619\u001b[0m             \u001b[0;34mf\"{COMBINED_UNSLOTH_NAME}_{model_type}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36mcreate_new_function\u001b[0;34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Direct module loading failed for {name}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Direct module loading failed for unsloth_compiled_module_gpt_oss: name 'KWARGS_TYPE' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-715345498.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m ] # More models at https://huggingface.co/unsloth\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unsloth/gpt-oss-20b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# None for auto detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m#     dispatch_model = FastGraniteModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             return FastModel.from_pretrained(\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0mmodel_name\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mmax_seq_length\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mredirector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mpatch_loss_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             model_types, supports_sdpa = unsloth_compile_transformers(\n\u001b[0m\u001b[1;32m    804\u001b[0m                 \u001b[0mdtype\u001b[0m                   \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0mmodel_name\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36munsloth_compile_transformers\u001b[0;34m(dtype, model_name, model_types, token, revision, trust_remote_code, sdpa_dynamic_mask, sdpa_bool_masks, sdpa_gqa_replace, sdpa_dynamic_compile, compile_attention, disable_causal_masks, compile_torch_modules, compile_custom_modules, compile_function_calls, fuse_lm_head, gradient_checkpointing, manual_replacements, fast_lora_forwards, fast_residual_stream, accurate_accumulation, epilogue_fusion, max_autotune, shape_padding, cudagraphs, debug, fullgraph, import_from_cache, disable, return_logits, unsloth_force_compile)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0msupports_sdpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m         _unsloth_compile_transformers(\n\u001b[0m\u001b[1;32m   1401\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m             \u001b[0msdpa_dynamic_mask\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0msdpa_dynamic_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36munsloth_compile_transformers\u001b[0;34m(model_type, sdpa_dynamic_mask, sdpa_bool_masks, sdpa_gqa_replace, sdpa_dynamic_compile, compile_attention, disable_causal_masks, compile_torch_modules, compile_custom_modules, compile_function_calls, fuse_lm_head, gradient_checkpointing, manual_replacements, fast_lora_forwards, fast_residual_stream, accurate_accumulation, epilogue_fusion, max_autotune, shape_padding, cudagraphs, debug, fullgraph, import_from_cache, disable, return_logits, supports_sdpa)\u001b[0m\n\u001b[1;32m   2628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mUNSLOTH_ENABLE_LOGGING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Direct module loading failed for unsloth_compiled_module_gpt_oss: name 'KWARGS_TYPE' is not defined"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
        "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
        "    \"unsloth/gpt-oss-120b\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gpt-oss-20b\",\n",
        "    dtype = dtype, # None for auto detection\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_section"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add_lora"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reasoning_effort_section"
      },
      "source": [
        "### Understanding Reasoning Effort Levels\n",
        "\n",
        "The `gpt-oss` models include a unique feature that allows you to adjust the model's **\"reasoning effort\"**‚Äîcontrolling the trade-off between performance and response speed (latency) by adjusting how many tokens the model uses to think.\n",
        "\n",
        "----\n",
        "\n",
        "Three distinct levels:\n",
        "* **Low**: Optimized for fast responses, minimal multi-step reasoning\n",
        "* **Medium**: Balanced performance and speed\n",
        "* **High**: Strongest reasoning performance for complex tasks (higher latency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_reasoning_low"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"low\",\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF-rYrnpVKnI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_medium_text"
      },
      "source": [
        "Changing the `reasoning_effort` to `high` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_reasoning_medium"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\",\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 2048, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_prep_section"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep: The Multilingual Reasoning Dataset\n",
        "\n",
        "We'll use the [HuggingFaceH4/Multilingual-Thinking](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking) dataset.\n",
        "- It contains reasoning chains translated into multiple languages (French, Spanish, German, Italian)\n",
        "- Each example has both the reasoning process (`analysis` channel) and final answer (`final` channel)\n",
        "- By training on this, the model learns to generate reasoning steps in different languages\n",
        "\n",
        "### Understanding the Harmony Format\n",
        "\n",
        "The GPT-OSS models use OpenAI's Harmony format for conversations. Here's what each field means:\n",
        "\n",
        "| Field | Purpose |\n",
        "|-------|---------|\n",
        "| `developer` | Custom instructions for the model (similar to system role) |\n",
        "| `user` | User's input question |\n",
        "| `assistant` | Model's output with two special channels |\n",
        "| `analysis` | The model's chain-of-thought reasoning |\n",
        "| `final` | The final response shown to the user |\n",
        "\n",
        "The key innovation: The assistant response contains **two channels**:\n",
        "- **`analysis` channel**: Where the model thinks step-by-step (can be in any language)\n",
        "- **`final` channel**: The polished response to the user\n",
        "\n",
        "This separation allows the model to reason in one language while responding in another!\n",
        "\n",
        "#### This notebook refers the HF_TOKEN from the Secrets tab. Please make sure the value is added for the HF_TOKEN before running the cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import os\n",
        "my_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"train\",token = my_token)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "format_text"
      },
      "source": [
        "To format our dataset, we will apply our version of the GPT OSS prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "format_dataset"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# Modify formatting_prompts_func to use the correct column names\n",
        "def formatting_prompts_func(examples):\n",
        "    # Assuming 'Context' is the user's message and 'Response' is the assistant's message\n",
        "    # We need to format this into the Harmony format expected by the model\n",
        "    texts = []\n",
        "    for i in range(len(examples['Context'])):\n",
        "        # Construct the conversation in the Harmony format\n",
        "        # This is a simplified example, you might need to adjust based on the dataset's actual structure\n",
        "        convo = [\n",
        "            {\"role\": \"user\", \"content\": examples['Context'][i]},\n",
        "            {\"role\": \"assistant\", \"channel\": \"final\", \"content\": examples['Response'][i]},\n",
        "            # If there's an analysis part in the dataset, you would add it here\n",
        "            # {\"role\": \"assistant\", \"channel\": \"analysis\", \"message\": \"...\"}\n",
        "        ]\n",
        "        texts.append(tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False))\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# The standardize_sharegpt function might not be necessary or might need adjustment\n",
        "# depending on how well it handles the new dataset structure.\n",
        "# Let's try applying our custom formatting first.\n",
        "# dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alFvOLunyzR9"
      },
      "source": [
        "### üîç Detecting and Inspecting Outliers\n",
        "\n",
        "When working with text datasets, some samples can be **much longer** than the majority.  \n",
        "These are called **outliers** and can negatively impact training (e.g., wasting compute on rare long sequences).\n",
        "\n",
        "We use the **Interquartile Range (IQR) method**:\n",
        "\n",
        "1. **Calculate quartiles (Q1, Q3)**  \n",
        "   - Q1 = 25th percentile  \n",
        "   - Q3 = 75th percentile  \n",
        "   - IQR = Q3 ‚àí Q1  \n",
        "\n",
        "2. **Define outlier threshold**  \n",
        "   - Any sample with length > Q3 + 1.5 √ó IQR is considered an outlier.  \n",
        "\n",
        "3. **Split dataset**  \n",
        "   - `cleaned_dataset`: all samples within the threshold  \n",
        "   - `outlier_dataset`: all samples above the threshold (potential outliers)  \n",
        "\n",
        "This way, we don‚Äôt just discard the long samples silently ‚Äî we **separate and inspect** them.  \n",
        "You can print or visualize `outlier_dataset` to understand whether these are valid data points (just long)  \n",
        "or noisy entries that should be removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oLUQpJiWoUR"
      },
      "outputs": [],
      "source": [
        "def check_token_lengths(dataset, tokenizer, text_key=\"text\", show_progress=True):\n",
        "    \"\"\"\n",
        "    Check the token lengths of all examples in a dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: HuggingFace Dataset or list of dicts with text entries.\n",
        "        tokenizer: HuggingFace tokenizer instance.\n",
        "        text_key: Key in dataset examples containing text (default \"text\").\n",
        "        show_progress: Whether to print lengths as we go.\n",
        "\n",
        "    Returns:\n",
        "        List of token lengths for all examples.\n",
        "    \"\"\"\n",
        "    token_lengths = []\n",
        "    for i, example in enumerate(dataset):\n",
        "        tokens = tokenizer(example[text_key], truncation=False)[\"input_ids\"]\n",
        "        length = len(tokens)\n",
        "        token_lengths.append(length)\n",
        "        if show_progress:\n",
        "            print(f\"Example {i}: {length} tokens\")\n",
        "    return token_lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkTNXhGcd_lF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def analyze_token_lengths(lengths, max_context=1024):\n",
        "    lengths = np.array(lengths)\n",
        "\n",
        "    print(f\"üìä Dataset size: {len(lengths)} examples\")\n",
        "    print(f\"Avg length: {lengths.mean():.1f}\")\n",
        "    print(f\"Median length: {np.median(lengths):.1f}\")\n",
        "    print(f\"Max length: {lengths.max()}\")\n",
        "\n",
        "    # Coverage at common cutoffs\n",
        "    for cutoff in [512, 1024, 2048, 4096]:\n",
        "        coverage = (lengths <= cutoff).mean() * 100\n",
        "        print(f\"‚â§ {cutoff} tokens: {coverage:.1f}% of examples\")\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(lengths, bins=50, color=\"steelblue\", edgecolor=\"black\")\n",
        "    plt.axvline(max_context, color=\"red\", linestyle=\"--\", label=f\"Model limit {max_context}\")\n",
        "    plt.xlabel(\"Token length\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Token length distribution\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xYrxTR4e-ak"
      },
      "outputs": [],
      "source": [
        "lengths = check_token_lengths(dataset, tokenizer, show_progress=False)\n",
        "analyze_token_lengths(lengths, max_context=1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS4_7EqQlaNv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Calculate quartiles\n",
        "q1 = np.percentile(lengths, 25)\n",
        "q3 = np.percentile(lengths, 75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Step 2: Define outlier threshold\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "print(f\"IQR upper bound: {upper_bound:.2f} tokens\")\n",
        "\n",
        "# Step 3: Separate clean vs outlier samples\n",
        "outlier_indices = [i for i, l in enumerate(lengths) if l > upper_bound]\n",
        "clean_indices = [i for i, l in enumerate(lengths) if l <= upper_bound]\n",
        "\n",
        "# Build datasets\n",
        "cleaned_dataset = dataset.select(clean_indices)\n",
        "outlier_dataset = dataset.select(outlier_indices)\n",
        "\n",
        "print(f\"Cleaned dataset size: {len(cleaned_dataset)} / {len(dataset)}\")\n",
        "print(f\"Outlier dataset size: {len(outlier_dataset)}\")\n",
        "\n",
        "# Peek at outliers\n",
        "for i in range(len(outlier_dataset)):\n",
        "    print(f\"\\n--- Outlier {i+1} ---\")\n",
        "    print(outlier_dataset[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi2W11d4lfii"
      },
      "outputs": [],
      "source": [
        "cleaned_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOVbRrGp1CHw"
      },
      "source": [
        "### ‚úÇÔ∏è Why We Need Chunking (with Special Tokens)\n",
        "\n",
        "Our dataset isn‚Äôt just raw text ‚Äî it includes **structured conversation tokens** such as:\n",
        "\n",
        "```\n",
        "\n",
        "<|start|>system<|message|> ... <|end|>\n",
        "<|start|>developer<|message|> ... <|end|>\n",
        "<|start|>user<|message|> ... <|end|>\n",
        "<|start|>assistant<|channel|>analysis<|message|> ... <|end|>\n",
        "<|start|>assistant<|channel|>final<|message|> ... <|end|>\n",
        "\n",
        "```\n",
        "\n",
        "These tokens **mark role boundaries** (system, developer, user, assistant) and **separate messages**.  \n",
        "They are **crucial for correct learning** ‚Äî if chunking cuts in the middle of them, the model may:\n",
        "\n",
        "- Miss a `<|start|>` or `<|end|>` marker, breaking the structure  \n",
        "- Lose track of **who is speaking** (system vs. user vs. assistant)  \n",
        "- Misinterpret instructions (e.g., reasoning language, special channels)  \n",
        "- Encounter incomplete sequences, leading to **NaN loss during training**  \n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è **The challenge:**  \n",
        "- We need to cap max sequence length at **1024 tokens** (because Colab‚Äôs T4 GPU runs out of memory at 2048).  \n",
        "- But if we na√Øvely truncate at 1024, we risk losing special tokens and breaking samples.  \n",
        "\n",
        "‚úÖ **The solution:**  \n",
        "- Use **chunking** into 1024-token windows.  \n",
        "- Ensure each chunk respects token boundaries and preserves `<|start|> ... <|end|>` structures.  \n",
        "- This way, even long samples (up to 32k tokens) are split into multiple valid training examples without losing critical role and instruction markers.  \n",
        "\n",
        "This keeps training **stable**, prevents NaNs, and ensures the model **actually learns the structured instruction format**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0stiz-EFzrjb"
      },
      "outputs": [],
      "source": [
        "cleaned_lengths = [len(tokenizer.encode(x[\"text\"])) for x in cleaned_dataset]\n",
        "\n",
        "# Find indices of samples between 2048 and 4096 tokens\n",
        "mid_long_indices = [i for i, l in enumerate(cleaned_lengths) if 2048 <= l <= 4096]\n",
        "\n",
        "print(f\"Found {len(mid_long_indices)} samples between 2048 and 4096 tokens\")\n",
        "\n",
        "# Peek at a few examples and show truncation\n",
        "for idx in mid_long_indices[:3]:  # pick 3 examples\n",
        "    text = cleaned_dataset[idx][\"text\"]\n",
        "    full_tokens = tokenizer.encode(text)\n",
        "\n",
        "    # Truncate\n",
        "    truncated_tokens = tokenizer.encode(text, max_length=1024, truncation=True)\n",
        "\n",
        "    print(f\"\\n--- Example {idx} ---\")\n",
        "    print(f\"Original length: {len(full_tokens)} tokens\")\n",
        "    print(f\"Truncated length: {len(truncated_tokens)} tokens (max=1024)\")\n",
        "\n",
        "    # Show kept vs discarded parts\n",
        "    print(\"\\n‚úÖ Truncated text (last 200 chars):\")\n",
        "    print(tokenizer.decode(truncated_tokens[-200:]))\n",
        "\n",
        "    discarded_tokens = full_tokens[1024:]\n",
        "    print(\"\\n‚ùå Discarded text\")\n",
        "    print(tokenizer.decode(discarded_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lebSYxLf-_d_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woYrNXA-mzzD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "END_OR_RETURN = r\"(?:<\\|end\\|>|<\\|return\\|>)\"\n",
        "MSG_BLOCK_RE = re.compile(rf\"<\\|start\\|>.*?{END_OR_RETURN}\", re.DOTALL)\n",
        "\n",
        "# header stays fine (captures up to <|message|>)\n",
        "MSG_HEADER_RE = re.compile(r\"^(.*?<\\|message\\|>)\", re.DOTALL)\n",
        "\n",
        "\n",
        "def find_message_blocks(text: str) -> List[str]:\n",
        "    return [m.group(0) for m in MSG_BLOCK_RE.finditer(text)]\n",
        "\n",
        "def split_long_message_block(\n",
        "    block_text: str,\n",
        "    tokenizer,\n",
        "    max_length: int = 2048,\n",
        "    overlap: int = 128,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Split one <|start|>...<|end|> block into multiple text chunks.\n",
        "    Preserves header (<|start|>role<|message|>) and closing <|end|> for each chunk.\n",
        "    \"\"\"\n",
        "    m = MSG_HEADER_RE.search(block_text)\n",
        "    if not m:\n",
        "        header_text, body_text, end_text = \"\", block_text, \"\"\n",
        "    else:\n",
        "        header_text = m.group(1)\n",
        "        rest = block_text[len(header_text):]\n",
        "        end_iter = list(re.finditer(END_OR_RETURN, rest))\n",
        "        if end_iter:\n",
        "            last = end_iter[-1]\n",
        "            body_text = rest[:last.start()]\n",
        "            end_text = last.group(0)           # either <|end|> or <|return|>\n",
        "        else:\n",
        "            body_text, end_text = rest, \"\"     # fallback\n",
        "\n",
        "    body_ids = tokenizer(body_text, add_special_tokens=False)[\"input_ids\"]\n",
        "    # available payload capacity (approximate by tokens)\n",
        "    capacity = max_length - len(tokenizer(header_text)[\"input_ids\"]) - len(tokenizer(end_text)[\"input_ids\"]) - 2\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    step = max(1, capacity - overlap)\n",
        "    while start < len(body_ids):\n",
        "        piece_ids = body_ids[start:start + capacity]\n",
        "        piece_text = tokenizer.decode(piece_ids, skip_special_tokens=False)\n",
        "        chunk_text = f\"{header_text}{piece_text}{end_text}\"\n",
        "        chunks.append(chunk_text)\n",
        "        if start + capacity >= len(body_ids):\n",
        "            break\n",
        "        start += step\n",
        "    return chunks\n",
        "\n",
        "def chunk_dialogue_to_text(\n",
        "    text: str,\n",
        "    tokenizer,\n",
        "    max_length: int = 2048,\n",
        "    overlap: int = 128,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Pack dialogue text (<|start|>...<|end|> blocks) into chunks of text only.\n",
        "    \"\"\"\n",
        "    blocks = find_message_blocks(text)\n",
        "    if not blocks:\n",
        "        return [text]\n",
        "\n",
        "    chunks, cur_text, cur_len = [], \"\", 0\n",
        "    for block in blocks:\n",
        "        block_len = len(tokenizer(block, add_special_tokens=False)[\"input_ids\"])\n",
        "        if cur_len + block_len <= max_length:\n",
        "            cur_text += block\n",
        "            cur_len += block_len\n",
        "        else:\n",
        "            if cur_text:\n",
        "                chunks.append(cur_text)\n",
        "            if block_len > max_length:\n",
        "                chunks.extend(split_long_message_block(block, tokenizer, max_length, overlap))\n",
        "                cur_text, cur_len = \"\", 0\n",
        "            else:\n",
        "                cur_text, cur_len = block, block_len\n",
        "    if cur_text:\n",
        "        chunks.append(cur_text)\n",
        "    return chunks\n",
        "\n",
        "def chunk_dataset_to_text(\n",
        "    dataset,\n",
        "    tokenizer,\n",
        "    text_key: str = \"text\",\n",
        "    max_length: int = 2048,\n",
        "    overlap: int = 128,\n",
        "    keep_fields: list[str] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert a dataset into boundary-aware text chunks.\n",
        "    Returns a HuggingFace Dataset with each row = {'text': chunk_text, ...}\n",
        "    \"\"\"\n",
        "    from datasets import Dataset\n",
        "    keep_fields = keep_fields or []\n",
        "\n",
        "    rows = []\n",
        "    for src_idx, example in enumerate(dataset):\n",
        "        text = example.get(text_key)\n",
        "        if not text:\n",
        "            continue\n",
        "        chunks = chunk_dialogue_to_text(text, tokenizer, max_length, overlap)\n",
        "        for chunk_id, ch_text in enumerate(chunks):\n",
        "            row = {\"text\": ch_text,\n",
        "                   \"source_index\": src_idx,\n",
        "                   \"chunk_id\": chunk_id,\n",
        "                   \"num_chunks\": len(chunks)}\n",
        "            for k in keep_fields:\n",
        "                row[k] = example.get(k)\n",
        "            rows.append(row)\n",
        "\n",
        "    return Dataset.from_list(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_VK7dmJAFjC"
      },
      "outputs": [],
      "source": [
        "CHUNKED = chunk_dataset_to_text(\n",
        "    cleaned_dataset,\n",
        "    tokenizer,\n",
        "    text_key=\"text\",\n",
        "    max_length=1024,\n",
        "    overlap=128,\n",
        "    keep_fields=[\"reasoning_language\",\"developer\",\"user\",\"analysis\",\"final\",\"messages\"],\n",
        ")\n",
        "\n",
        "print(CHUNKED)\n",
        "# => Dataset({ features: ['text','source_index','chunk_id','num_chunks',...], num_rows: ... })\n",
        "\n",
        "# sanity check\n",
        "print(CHUNKED[0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqMygCEm-Nm1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eyRYWlKw-RH_"
      },
      "outputs": [],
      "source": [
        "DATASET_REPO = \"Scropo/MindFull-AI\"\n",
        "CHUNKED.push_to_hub(DATASET_REPO,token=my_token,create_pr=1)\n",
        "# https://huggingface.co/datasets/LLMImplementation/multilingual-thinking-cleaned-chunked-1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_FCMlUnU0t5"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's fine-tune our model, teaching it to respond with empathy and offer supportive guidance. We'll run for just 30 steps to keep this demo quick, but for a full training, you can set `num_train_epochs=1` and remove `max_steps`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_trainer"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "args = SFTConfig(\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,   # effective batch size = 4\n",
        "    num_train_epochs = 1,          # Set this for 1 full training run.\n",
        "    packing = False,\n",
        "    learning_rate = 5e-5,              # safer for longer runs than 2e-4\n",
        "    warmup_steps = 15,                 # ~6% of total steps, smoother ramp\n",
        "    lr_scheduler_type = \"cosine\",      # better for longer schedules\n",
        "    max_grad_norm = 0.5,               # strong clipping, avoids NaNs\n",
        "    weight_decay = 0.01,\n",
        "    optim = \"adamw_torch\",             # stick to stable optimizer first\n",
        "    logging_steps = 1,\n",
        "    save_steps = 50,                   # optional: checkpointing every 50 steps\n",
        "    fp16 = False,                      # keep off until stable\n",
        "    bf16 = False,                      # enable later if GPU supports\n",
        "    seed = 3407,\n",
        "    output_dir = \"outputs\",\n",
        "    report_to = \"none\",\n",
        "    max_seq_length = 1024,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = CHUNKED,\n",
        "    args = args,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_memory"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_final_memory"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** Currently finetunes can only be loaded via Unsloth in the meantime - we're working on vLLM and GGUF exporting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0j_HrsA1pSO"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"finetuned_model\")\n",
        "model.push_to_hub(\"Scropo/MindFull-AI\", token = my_token ) # Save to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKoTVXyM1tgt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference_section"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference: Testing Empathetic Responses\n",
        "Let's see how our fine-tuned model responds to a user expressing feelings of stress or anxiety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_french"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can provide mental health counseling.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I've been feeling really down lately and can't seem to find joy in anything. What should I do?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\", # Changed to high for more detailed reasoning\n",
        ").to(model.device)\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(**inputs, max_new_tokens = 1048, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-3fLuBzuITm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_german"
      },
      "outputs": [],
      "source": [
        "# Test with German reasoning\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can provide mental health counseling.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I feel anxious before exams, what can I do?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"medium\",\n",
        ").to(model.device)\n",
        "_ = model.generate(**inputs, max_new_tokens = 1024, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uRDgkv74-3-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_UqIwlYeBzZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nCG5nQ1eB2r"
      },
      "outputs": [],
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can provide mental health counseling.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How can I improve my sleep routine?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"medium\",\n",
        ").to(model.device)\n",
        "_ = model.generate(**inputs, max_new_tokens = 1024, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_untrained_text"
      },
      "source": [
        "### Testing with Prompts that was Not in Training Data\n",
        "\n",
        "Interestingly, the model can even attempt reasoning in prompt it wasn't explicitly trained on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_japanese"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can provide mental health counseling.\"},\n",
        "    {\"role\": \"user\", \"content\": \"My mom has Alzheimer's, and I've been her primary caregiver for the past few years. I don't know what to do, Can you help?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"medium\",\n",
        ").to(model.device)\n",
        "_ = model.generate(**inputs, max_new_tokens = 1024, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reload_text"
      },
      "source": [
        "### To run the finetuned model, you can do the below after setting `if False` to `if True` in a new instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "reload_model"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  from unsloth import FastLanguageModel\n",
        "  from peft import PeftModel\n",
        "\n",
        "  max_seq_length = 1024\n",
        "  dtype=None\n",
        "  # 1) load the same base\n",
        "  base, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name = \"unsloth/gpt-oss-20b\",\n",
        "      dtype = dtype, # None for auto detection\n",
        "      max_seq_length = max_seq_length, # Choose any for long context!\n",
        "      load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "      full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "      # token = \"hf_...\", # use one if using gated models\n",
        "  )\n",
        "\n",
        "  # 2) attach the adapter\n",
        "  model = PeftModel.from_pretrained(base, \"finetuned_model\")\n",
        "\n",
        "  # 3) enable Unsloth‚Äôs fast inference path\n",
        "  FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjMTCr1W71DA"
      },
      "source": [
        "## Or you can load the base model and then attach the adapters from your Hub repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sARBK_SXjpVT"
      },
      "outputs": [],
      "source": [
        "\n",
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    from peft import PeftModel\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    BASE_ID    = \"unsloth/gpt-oss-20b\"\n",
        "    ADAPTER_ID = \"Scropo/MindFull-AI\"  # your Hub repo\n",
        "\n",
        "    max_seq_length = 1024\n",
        "\n",
        "    # 1) Load the same base model you trained against (4-bit is T4-friendly)\n",
        "    base, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name      = BASE_ID,\n",
        "        dtype           = None,          # auto\n",
        "        max_seq_length  = max_seq_length,\n",
        "        load_in_4bit    = True,          # QLoRA-style inference\n",
        "        # token        = \"hf_...\"        # if base is gated or your account is private\n",
        "    )\n",
        "\n",
        "    # (Optional) If you pushed the tokenizer to the adapter repo, prefer it:\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(ADAPTER_ID, use_fast=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Attach LoRA adapters directly from the Hub\n",
        "    model = PeftModel.from_pretrained(base, ADAPTER_ID)\n",
        "\n",
        "    # 3) Enable Unsloth‚Äôs fast inference path\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    # Quick sanity test\n",
        "    prompt = \"<|start|>user<|message|>¬øCu√°l es el capital de Australia?<|end|>\\n<|start|>assistant<|channel|>analysis<|message|>\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## üèÅ Conclusion\n",
        "\n",
        "You‚Äôve successfully fine-tuned a base language model to act as an **empathetic mental well-being assistant**. Key takeaways:\n",
        "\n",
        "1.  **Default Bias to Facts:** Large language models are typically tuned for factual recall and instruction-following, often lacking the softness and empathy required for supportive conversations.\n",
        "2.  **Empathetic Fine-Tuning Matters:** Training on a curated dataset of therapeutic conversations and empathetic dialogue is crucial to steer the model towards safe, helpful, and emotionally intelligent responses.\n",
        "3.  **Structured Formatting is Key:** Using a format that separates an `internal_strategy` from the final `response` helps the model learn to \"think\" about validation and empathy before generating its user-facing message.\n",
        "4.  **Resource-aware Training:** On a consumer GPU like a Colab **T4 (16 GB)**, using techniques like **LoRA/QLoRA**, a moderate `max_seq_length=1024`, and small batch sizes are essential to train effectively without running out of memory.\n",
        "5.  **Data Quality is Paramount:** For a sensitive application like this, carefully curating, cleaning, and anonymizing the dataset is the most critical step to ensure model safety and prevent harmful outputs.\n",
        "\n",
        "**What‚Äôs next (brief):** Implement proper **train/validation/test splits** to prevent data leakage. Beyond technical loss metrics, create a qualitative test set of challenging prompts to manually review model responses for safety and tone. Always evaluate thoroughly before considering any real-world application.\n",
        "\n",
        "-----\n",
        "\n",
        "### üìö References\n",
        "\n",
        "  - Unsloth Llama 3 fine-tuning tutorial: [https://github.com/unslothai/unsloth/blob/main/notebooks/Llama\\_3\\_8b\\_Instruct\\_Tuning.ipynb](https://www.google.com/search?q=https://github.com/unslothai/unsloth/blob/main/notebooks/Llama_3_8b_Instruct_Tuning.ipynb)\n",
        "  - Hugging Face Fine-tuning Guide: [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19519d18",
        "outputId": "ae559983-c9e8-4ba7-9231-d96280e7d827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "46562d88d09a435e8a4d06bd6aff592a",
            "52211a8d91414169a22309b9324dd8c8",
            "cf0d2d1fcfab403fa6a0f618a14e7eb5",
            "7249ab3d9b014ca4956a2f80da546140",
            "3ab03a294d32481bbfe1a4f453f63d35",
            "382396f348c2422ba3b4a50e712144a1",
            "4852ddb419e04111bd0822a1906bbf48",
            "15fd695302e54e27801d2c39b48fa904",
            "fccbf0d4f0c94e0687ce60f4f00cd276",
            "e9d6aeedeaa8494fb293290d9a55a8d7",
            "6eea12f4693f426789aa8475f58716ed",
            "d6ca639424a349329162bc5372b0070d",
            "8b8dced5e984434c92196348e14bc9c1",
            "16d29279c26141409d535f67ebb12536",
            "16951bd97ba94e248fbdbe36d96b916e",
            "70c2ec0d0eb2486bb589405af320e511",
            "b917b3ba19c64943bf8d914abe251cd0"
          ]
        }
      },
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=True) # Use new_session=True to ensure a fresh login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46562d88d09a435e8a4d06bd6aff592a"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "-_FCMlUnU0t5",
        "save_section",
        "inference_section",
        "test_untrained_text",
        "reload_text",
        "BjMTCr1W71DA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46562d88d09a435e8a4d06bd6aff592a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52211a8d91414169a22309b9324dd8c8",
              "IPY_MODEL_cf0d2d1fcfab403fa6a0f618a14e7eb5",
              "IPY_MODEL_7249ab3d9b014ca4956a2f80da546140",
              "IPY_MODEL_3ab03a294d32481bbfe1a4f453f63d35",
              "IPY_MODEL_382396f348c2422ba3b4a50e712144a1"
            ],
            "layout": "IPY_MODEL_4852ddb419e04111bd0822a1906bbf48"
          }
        },
        "52211a8d91414169a22309b9324dd8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15fd695302e54e27801d2c39b48fa904",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fccbf0d4f0c94e0687ce60f4f00cd276",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "cf0d2d1fcfab403fa6a0f618a14e7eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e9d6aeedeaa8494fb293290d9a55a8d7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6eea12f4693f426789aa8475f58716ed",
            "value": ""
          }
        },
        "7249ab3d9b014ca4956a2f80da546140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_d6ca639424a349329162bc5372b0070d",
            "style": "IPY_MODEL_8b8dced5e984434c92196348e14bc9c1",
            "value": true
          }
        },
        "3ab03a294d32481bbfe1a4f453f63d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_16d29279c26141409d535f67ebb12536",
            "style": "IPY_MODEL_16951bd97ba94e248fbdbe36d96b916e",
            "tooltip": ""
          }
        },
        "382396f348c2422ba3b4a50e712144a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c2ec0d0eb2486bb589405af320e511",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b917b3ba19c64943bf8d914abe251cd0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "4852ddb419e04111bd0822a1906bbf48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "15fd695302e54e27801d2c39b48fa904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fccbf0d4f0c94e0687ce60f4f00cd276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9d6aeedeaa8494fb293290d9a55a8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eea12f4693f426789aa8475f58716ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6ca639424a349329162bc5372b0070d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8dced5e984434c92196348e14bc9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16d29279c26141409d535f67ebb12536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16951bd97ba94e248fbdbe36d96b916e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "70c2ec0d0eb2486bb589405af320e511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b917b3ba19c64943bf8d914abe251cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}